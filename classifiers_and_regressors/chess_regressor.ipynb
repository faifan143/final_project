{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def generate_storm_chess_stats():\n",
    "  \"\"\"Generates a random storm chess stats dataset with the following constraints:\n",
    "\n",
    "  * The total number of challenges is 10.\n",
    "  * There are only 3 available hints during each challenge.\n",
    "  * The maximum time for each challenge is 10 minutes.\n",
    "\n",
    "  Returns:\n",
    "    A Pandas DataFrame containing the generated data.\n",
    "  \"\"\"\n",
    "\n",
    "  # Generate random player and agent Elo ratings.\n",
    "  player_elo = np.random.randint(700, 3000, )\n",
    "  agent_elo = np.random.randint(700, 3000, )\n",
    "\n",
    "  # Generate random number of solved storm chess challenges out of 10.\n",
    "  solved = np.random.randint(0, 10, )\n",
    "\n",
    "  # Generate random average completion time for each of the solved challenges (in minutes).\n",
    "  avg_time = np.random.randint(1, 10,)\n",
    "\n",
    "  # Generate random average number of hints used for each of the solved challenges.\n",
    "  avg_hints = np.random.randint(0, 3,)\n",
    "\n",
    "  # Generate random number of skipped challenges out of the 10 challenges.\n",
    "  skipped = np.random.randint(0, 10,)\n",
    "\n",
    "  # Calculate the expected score for each player based on their Elo ratings.\n",
    "  expected_score = 1 / (1 + 10**((agent_elo - player_elo) / 400))\n",
    "\n",
    "  # Calculate the actual score for each player based on the number of solved challenges.\n",
    "  actual_score = solved / 10\n",
    "\n",
    "  # Calculate the new Elo rating for each player.\n",
    "  K = 32 if player_elo < 2400 else 16\n",
    "  new_elo = round(player_elo + K * (expected_score - actual_score))\n",
    "\n",
    "  return [player_elo , agent_elo , solved , avg_hints , avg_time  , skipped , new_elo]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  dataset = []\n",
    "  for sample in range(1000000):\n",
    "    dataset.append(generate_storm_chess_stats())\n",
    "  df = pd.DataFrame(dataset,columns=[\"elo\", \"agent_elo\", \"solved\", \"avg_hints\", \"avg_time\", \"skipped\", \"new_elo\"])\n",
    "  # Save the dataset to a CSV file.\n",
    "  df.to_csv(\"C:\\\\Users\\\\faisa\\\\Documents\\\\Games_Section\\\\chess\\\\CSVs\\\\storm_chess_stats.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\faisa\\\\Documents\\\\Games_Section\\\\chess\\\\CSVs\\\\storm_chess_stats.csv\")\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop(columns=[\"new_elo\"])\n",
    "y = data[\"new_elo\"]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create a RandomForestRegressor object\n",
    "model = RandomForestRegressor(n_estimators=150, max_depth=100, n_jobs=-1)\n",
    "\n",
    "# Fit the RandomForestRegressor object to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "joblib.dump(model, \"storm_chess_regressor.pkl\")\n",
    "\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "print(f\"R-squared score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\faisa\\\\Documents\\\\Games_Section\\\\chess\\\\CSVs\\\\storm_chess_stats.csv\")\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop(columns=[\"new_elo\"])\n",
    "y = data[\"new_elo\"]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "type(X_test.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://10.23.0.2:5001/ (Press CTRL+C to quit)\n",
      "192.168.1.7 - - [28/Oct/2023 23:36:58] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:39] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:40] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:40] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:41] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:42] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:42] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:43] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:43] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:44] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:45] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:45] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:46] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:46] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:46] \"POST /nnue_policy HTTP/1.1\" 200 -\n",
      "192.168.1.7 - - [28/Oct/2023 23:37:46] \"POST /nnue_policy HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import stockfish\n",
    "import chess\n",
    "\n",
    "model = joblib.load(\"storm_chess_regressor.pkl\")\n",
    "stockfish_path = \"C:\\\\Users\\\\faisa\\\\Documents\\\\Games_Section\\\\chess\\\\AIs\\\\stockfish\\\\stockfish-windows-x86-64-avx2.exe\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/predict_elo\", methods=[\"POST\"])\n",
    "def predict_elo():\n",
    "    features = request.get_json()[\"features\"]\n",
    "\n",
    "    new_data = pd.DataFrame(features)\n",
    "    predicted_elo = model.predict(new_data)\n",
    "    \n",
    "    predicted_elo_string = str(predicted_elo)\n",
    "    \n",
    "    return jsonify({\"predicted_level\": predicted_elo_string})\n",
    "\n",
    "\n",
    "@app.route(\"/nnue_eval\", methods=[\"POST\"])\n",
    "def nnue_evaluate():\n",
    "    fen = request.get_json()[\"board\"]\n",
    "    engine = stockfish.Stockfish(stockfish_path)\n",
    "    engine.set_fen_position(fen)\n",
    "    evaluation = engine.get_evaluation()\n",
    "    return jsonify({\"evaluation\": evaluation['value']})\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/nnue_policy\", methods=[\"POST\"])\n",
    "def nnue_policy_network():\n",
    "  \"\"\"Returns a probability distribution over the set of legal moves.\"\"\"\n",
    "  fen = request.get_json()[\"fen\"]\n",
    "  # Create a Stockfish engine\n",
    "  engine = stockfish.Stockfish(stockfish_path)\n",
    "\n",
    "  # Set the FEN position in the Stockfish engine\n",
    "  engine.set_fen_position(fen)\n",
    "\n",
    "  # Get the best move from Stockfish\n",
    "  best_move = engine.get_best_move()\n",
    "  \"\"\"\n",
    "  # Get the list of legal moves\n",
    "  board = chess.Board(fen)\n",
    "  legal_moves = list(board.legal_moves)\n",
    "\n",
    "  # Calculate the probability of each legal move\n",
    "  policy_probabilities = {}\n",
    "  for move in legal_moves:\n",
    "    policy_probabilities[move] = 1.0 if move == best_move else 0.0\n",
    "\n",
    "  # Normalize the policy probabilities\n",
    "  sum_of_probabilities = sum(policy_probabilities.values())\n",
    "  for move in legal_moves:\n",
    "    policy_probabilities[move] /= sum_of_probabilities\n",
    "\n",
    "  \"\"\"  \n",
    "  # Return the policy probabilities\n",
    "  return jsonify({\"best_move\": best_move})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stockfish\n",
    "import chess\n",
    "\n",
    "stockfish_path = \"C:\\\\Users\\\\faisa\\\\Documents\\\\Games_Section\\\\chess\\\\AIs\\\\stockfish\\\\stockfish-windows-x86-64-avx2.exe\"\n",
    "\n",
    "def nnue_policy_network(fen):\n",
    "  \n",
    "  engine = stockfish.Stockfish(stockfish_path)\n",
    "\n",
    "  engine.set_fen_position(fen)\n",
    "\n",
    "  best_move = engine.get_best_move()\n",
    "\n",
    "  board = chess.Board(fen)\n",
    "\n",
    "  legal_moves = list(board.legal_moves)\n",
    "  l_moves = [str(move) for move in legal_moves]\n",
    "\n",
    "\n",
    "  policy_probabilities = {}\n",
    "  for move in l_moves:\n",
    "    policy_probabilities[move] = 1.0 if move == best_move else 0.0\n",
    "\n",
    "  \n",
    "  sum_of_probabilities = sum(policy_probabilities.values())\n",
    "  for move in l_moves:\n",
    "    policy_probabilities[move] /= sum_of_probabilities\n",
    "\n",
    "  return policy_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a8g8': 0.0, 'a8f8': 0.0, 'a8e8': 0.0, 'a8d8': 0.0, 'a8c8': 0.0, 'a8b8': 0.0, 'e7e8': 0.0, 'e7g7': 0.0, 'e7f7': 0.0, 'e7d7': 0.0, 'e7c7': 0.0, 'e7e6': 0.0, 'f2b6': 0.0, 'f2c5': 0.0, 'f2d4': 0.0, 'f2g3': 0.0, 'f2e3': 0.0, 'f2g1': 0.0, 'f2e1': 0.0, 'b2e5': 0.0, 'b2d4': 0.0, 'b2c3': 0.0, 'b2b3': 0.0, 'b2a3': 0.0, 'b2c2': 0.0, 'b2a2': 0.0, 'b2c1': 0.0, 'b2b1': 1.0, 'b2a1': 0.0, 'b7b6': 0.0, 'a7a6': 0.0, 'f6f5': 0.0, 'd5d4': 0.0, 'b7b5': 0.0, 'a7a5': 0.0}\n"
     ]
    }
   ],
   "source": [
    "x = nnue_policy_network(\"r6k/pp2r2p/4Rp1Q/3p4/8/1N1P2R1/PqP2bPP/7K b - - 0 24\")\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
